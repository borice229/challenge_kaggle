{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5adf9772-0137-446b-829c-f615fe385a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importation des libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2681aa2a-889e-4a8a-bd76-02819382b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = pd.read_csv(\"data_clean.csv\", sep=\",\")\n",
    "data = data_clean.drop(columns=['Y_lag_1','Y_rolling_mean_6','Y_rolling_std_6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2011e975-9b7a-42fd-afce-6bc5e878148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "45964ecc-853a-4828-8a61-e0fb56820eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n'est pas excuter\n",
    "numeric_data = data.select_dtypes(include=['number'])\n",
    "Q1 = numeric_data.quantile(0.25)\n",
    "Q3 = numeric_data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "data_filtered = data[~((numeric_data < (Q1 - 1.5 * IQR)) | \n",
    "                       (numeric_data > (Q3 + 1.5 * IQR))).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7ed0c9e9-e305-495a-bc63-e224719b00c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "# Conversion des colonnes en variables catégorielles\n",
    "data_filtered['hour'] = data_filtered['hour'].astype('category')\n",
    "data_filtered['weekday'] = data_filtered['weekday'].astype('category')\n",
    "data_filtered['month'] = data_filtered['month'].astype('category')\n",
    "data_filtered['is_weekend'] = data_filtered['is_weekend'].astype('category')\n",
    "# Étape 1 : Encoder les colonnes catégoriques\n",
    "data_with_dummies = pd.get_dummies(\n",
    "    data_filtered, \n",
    "    columns=['hour', 'weekday', 'month', 'is_weekend'], \n",
    "    drop_first=True\n",
    ")\n",
    "# Convertir toutes les colonnes booléennes en entiers\n",
    "data_with_dummies = data_with_dummies.astype({col: 'int' for col in data_with_dummies.select_dtypes('bool').columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68882a2-6610-4e37-b85a-868811ad6d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_dummies.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c8bfbae9-631a-468f-a496-237b25738931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tri des données pour respecter l'ordre chronologique\n",
    "datan = data_with_dummies.sort_values(by='DELIVERY_START').reset_index(drop=True)\n",
    "\n",
    "# Séparation chronologique des ensembles\n",
    "train_size = 0.6\n",
    "test_1_size = 0.2\n",
    "\n",
    "n = len(datan)\n",
    "train_end = int(n * train_size)\n",
    "test_1_end = train_end + int(n * test_1_size)\n",
    "#les variables pour le random\n",
    "train_f = data_filtered.iloc[:train_end]\n",
    "test_1_f = data_filtered.iloc[train_end:test_1_end]\n",
    "test_2_f = data_filtered.iloc[test_1_end:]\n",
    "\n",
    "\n",
    "train = datan.iloc[:train_end]\n",
    "test_1 = datan.iloc[train_end:test_1_end]\n",
    "test_2 = datan.iloc[test_1_end:]\n",
    "\n",
    "# Définir les variables indépendantes et dépendantes\n",
    "X_train = train.drop(columns=['spot_id_delta', 'DELIVERY_START'])\n",
    "y_train = train['spot_id_delta']\n",
    "X_test1 = test_1.drop(columns=['spot_id_delta', 'DELIVERY_START'])\n",
    "y_test1 = test_1['spot_id_delta']\n",
    "X_test2 = test_2.drop(columns=['spot_id_delta', 'DELIVERY_START'])\n",
    "y_test2 = test_2['spot_id_delta']\n",
    "\n",
    "# Définir les variables indépendantes et dépendantes\n",
    "X_train_f = train_f.drop(columns=['spot_id_delta', 'DELIVERY_START'])\n",
    "y_train_f = train_f['spot_id_delta']\n",
    "X_test1_f = test_1_f.drop(columns=['spot_id_delta', 'DELIVERY_START'])\n",
    "y_test1_f = test_1_f['spot_id_delta']\n",
    "X_test2_f = test_2_f.drop(columns=['spot_id_delta', 'DELIVERY_START'])\n",
    "y_test2_f = test_2_f['spot_id_delta']\n",
    "\n",
    "# Normalisation des données\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test1_scaled = scaler.transform(X_test1)\n",
    "X_test2_scaled = scaler.transform(X_test2)\n",
    "\n",
    "# Fonction d'évaluation des métriques avec affichage clair\n",
    "def evaluate_model(y_true, y_pred, model_name, dataset_name):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    sign_accuracy = np.mean(np.sign(y_true) == np.sign(y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"=== {model_name} - {dataset_name} ===\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "    print(f\"Sign Accuracy: {sign_accuracy:.2f}\")\n",
    "    print(f\"R² Score: {r2:.2f}\\n\")\n",
    "    return mse, mae, sign_accuracy, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bfcefe-f637-470f-be2f-2b2b6c9db9f9",
   "metadata": {},
   "source": [
    "# Model\n",
    "__Random forest__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "beffb1af-4cdd-4e81-a5bf-a0b1149d0d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout,LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d5bd8581-7d9e-4428-9b31-98becc7cb92b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Forêt Aléatoire - Train ===\n",
      "Mean Squared Error (MSE): 2420.14\n",
      "Mean Absolute Error (MAE): 26.05\n",
      "Sign Accuracy: 0.50\n",
      "R² Score: -0.16\n",
      "\n",
      "=== Forêt Aléatoire - Test 1 ===\n",
      "Mean Squared Error (MSE): 1207.39\n",
      "Mean Absolute Error (MAE): 27.79\n",
      "Sign Accuracy: 0.51\n",
      "R² Score: -0.59\n",
      "\n",
      "=== Forêt Aléatoire - Test 2 ===\n",
      "Mean Squared Error (MSE): 623.90\n",
      "Mean Absolute Error (MAE): 18.44\n",
      "Sign Accuracy: 0.56\n",
      "R² Score: -1.26\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(623.8964944532585),\n",
       " np.float64(18.44474553391911),\n",
       " np.float64(0.5622032288698955),\n",
       " -1.2569302619424918)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Modèle de Forêt Aléatoire\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=500,  # Augmentation des arbres\n",
    "    max_depth=20,  # Profondeur ajustée\n",
    "    min_samples_split=5,  # Réduction du seuil pour splits\n",
    "    min_samples_leaf=6,  # Feuilles plus petites\n",
    "    random_state=42,\n",
    "    bootstrap=True,\n",
    ")\n",
    "rf_model.fit(X_train_f, y_train_f)\n",
    "\n",
    "# Prédictions Forêt Aléatoire\n",
    "y_pred_rf_train = rf_model.predict(X_train_f)\n",
    "y_pred_rf_test1 = rf_model.predict(X_test1_f)\n",
    "y_pred_rf_test2 = rf_model.predict(X_test2_f)\n",
    "\n",
    "# Évaluation des métriques\n",
    "# Forêt Aléatoire\n",
    "evaluate_model(y_train, y_pred_rf_train, \"Forêt Aléatoire\", \"Train\")\n",
    "evaluate_model(y_test1, y_pred_rf_test1, \"Forêt Aléatoire\", \"Test 1\")\n",
    "evaluate_model(y_test2, y_pred_rf_test2, \"Forêt Aléatoire\", \"Test 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8717ff5b-e94a-4771-828d-783bd08d4266",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Feature  Importance\n",
      "3        nucelear_power_available    0.187138\n",
      "0                   load_forecast    0.160845\n",
      "8            predicted_spot_price    0.102421\n",
      "4    wind_power_forecasts_average    0.094392\n",
      "15                    sin_weekday    0.076014\n",
      "6        wind_power_forecasts_std    0.062274\n",
      "10                        weekday    0.054843\n",
      "7       solar_power_forecasts_std    0.033042\n",
      "17                      sin_month    0.029349\n",
      "19                   Y_gas_mean_6    0.027613\n",
      "9                            hour    0.024393\n",
      "2             gas_power_available    0.021986\n",
      "16                    cos_weekday    0.021537\n",
      "5   solar_power_forecasts_average    0.020671\n",
      "20                    Y_gas_std_6    0.018823\n",
      "13                       sin_hour    0.015394\n",
      "21                  Y_coal_mean_6    0.012167\n",
      "11                          month    0.011124\n",
      "1            coal_power_available    0.008897\n",
      "18                      cos_month    0.006831\n",
      "14                       cos_hour    0.005797\n",
      "22                   Y_coal_std_6    0.002754\n",
      "12                     is_weekend    0.001696\n"
     ]
    }
   ],
   "source": [
    "# Importance des variables pour Random Forest\n",
    "importances = rf_model.feature_importances_\n",
    "feature_names = X_train_f.columns\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "print(importance_df.sort_values(by='Importance', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d0f8464d-912d-4ea5-9867-bf4b4dc56dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les variables avec une importance proche de 0\n",
    "threshold = 0.02\n",
    "low_importance_features = importance_df[importance_df['Importance'] < threshold]['Feature']\n",
    "X_train_reduced = X_train_f.drop(columns=low_importance_features)\n",
    "X_test1_reduced = X_test1_f.drop(columns=low_importance_features)\n",
    "X_test2_reduced = X_test2_f.drop(columns=low_importance_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a225fa6-b8bf-4c88-ad13-b1c1ffc27f66",
   "metadata": {},
   "source": [
    "__Apres suppression__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f66b8c9b-0307-410d-a2ae-2233d4efdd32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6318, 18)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "103ac661-ba5c-4cef-a76e-7fbdb4940fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Forêt Aléatoire - Train ===\n",
      "Mean Squared Error (MSE): 1995.94\n",
      "Mean Absolute Error (MAE): 22.68\n",
      "Sign Accuracy: 0.51\n",
      "R² Score: -0.53\n",
      "\n",
      "=== Forêt Aléatoire - Test 1 ===\n",
      "Mean Squared Error (MSE): 1776.33\n",
      "Mean Absolute Error (MAE): 29.56\n",
      "Sign Accuracy: 0.48\n",
      "R² Score: -0.28\n",
      "\n",
      "=== Forêt Aléatoire - Test 2 ===\n",
      "Mean Squared Error (MSE): 2151.65\n",
      "Mean Absolute Error (MAE): 30.15\n",
      "Sign Accuracy: 0.52\n",
      "R² Score: -0.07\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(2151.645309555837),\n",
       " np.float64(30.153674293855886),\n",
       " np.float64(0.5242165242165242),\n",
       " -0.07444750134562317)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Modèle de Forêt Aléatoire\n",
    "rf_model_f = RandomForestRegressor(\n",
    "    n_estimators=1000,\n",
    "    max_depth=15,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=4,\n",
    "    random_state=42,\n",
    "    bootstrap=True\n",
    ")\n",
    "\n",
    "rf_model_f.fit(X_train_reduced, y_train)\n",
    "\n",
    "# Prédictions Forêt Aléatoire\n",
    "y_pred_rf_train_reduced = rf_model_f.predict(X_train_reduced)\n",
    "y_pred_rf_test1_reduced = rf_model_f.predict(X_test1_reduced)\n",
    "y_pred_rf_test2_reduced = rf_model_f.predict(X_test2_reduced)\n",
    "\n",
    "# Évaluation des métriques\n",
    "# Forêt Aléatoire\n",
    "evaluate_model(y_train_f, y_pred_rf_train_reduced, \"Forêt Aléatoire\", \"Train\")\n",
    "evaluate_model(y_test1_f, y_pred_rf_test1_reduced, \"Forêt Aléatoire\", \"Test 1\")\n",
    "evaluate_model(y_test2_f, y_pred_rf_test2_reduced, \"Forêt Aléatoire\", \"Test 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9290ea68-1313-4558-ab3e-954453605e6e",
   "metadata": {},
   "source": [
    "__Reseau neurones__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f09a1bf-f282-4ef3-8ab1-83c75826f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.losses import Huber\n",
    "\n",
    "# Modèle optimisé\n",
    "nn_model = Sequential([\n",
    "    Dense(64, input_dim=X_train_scaled.shape[1], activation='selu', kernel_regularizer='l2'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='selu', kernel_regularizer='l2'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "nn_model.compile(optimizer=Adam(learning_rate=0.00005), loss=Huber(delta=1.0))\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Entraînement\n",
    "nn_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_test1_scaled, y_test1),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Prédictions Réseau de Neurones\n",
    "y_pred_nn_train = nn_model.predict(X_train_scaled).flatten()\n",
    "y_pred_nn_test1 = nn_model.predict(X_test1_scaled).flatten()\n",
    "y_pred_nn_test2 = nn_model.predict(X_test2_scaled).flatten()\n",
    "\n",
    "\n",
    "# Réseau de Neurones\n",
    "evaluate_model(y_train, y_pred_nn_train, \"Réseau de Neurones\", \"Train\")\n",
    "evaluate_model(y_test1, y_pred_nn_test1, \"Réseau de Neurones\", \"Test 1\")\n",
    "evaluate_model(y_test2, y_pred_nn_test2, \"Réseau de Neurones\", \"Test 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc31473-f95e-4a27-a735-d045fa47f96a",
   "metadata": {},
   "source": [
    "__Boosting__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "240543d5-075e-4b22-8c58-073223cfcf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Gradient Boosting - Train ===\n",
      "Mean Squared Error (MSE): 1344.72\n",
      "Mean Absolute Error (MAE): 15.61\n",
      "Sign Accuracy: 0.50\n",
      "R² Score: -0.03\n",
      "\n",
      "=== Gradient Boosting - Test 1 ===\n",
      "Mean Squared Error (MSE): 1417.52\n",
      "Mean Absolute Error (MAE): 25.63\n",
      "Sign Accuracy: 0.48\n",
      "R² Score: -0.02\n",
      "\n",
      "=== Gradient Boosting - Test 2 ===\n",
      "Mean Squared Error (MSE): 2010.79\n",
      "Mean Absolute Error (MAE): 27.70\n",
      "Sign Accuracy: 0.49\n",
      "R² Score: -0.00\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(2010.7925032150767),\n",
       " np.float64(27.69574017607925),\n",
       " np.float64(0.49382716049382713),\n",
       " -0.004111119620333392)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Modèle Gradient Boosting\n",
    "gb_model_f = GradientBoostingRegressor(\n",
    "    n_estimators=700,  # Nombre d'arbres dans le modèle\n",
    "    learning_rate=0.001,  # Taux d'apprentissage\n",
    "    #max_depth=20,         # Profondeur maximale des arbres\n",
    "    min_samples_split=10,  # Nombre minimum d'échantillons pour diviser un noeud\n",
    "    min_samples_leaf=4,    # Nombre minimum d'échantillons dans une feuille\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Entraînement du modèle\n",
    "gb_model_f.fit(X_train_f, y_train)\n",
    "\n",
    "# Prédictions\n",
    "y_pred_gb_train_reduced = gb_model_f.predict(X_train_f)\n",
    "y_pred_gb_test1_reduced = gb_model_f.predict(X_test1_f)\n",
    "y_pred_gb_test2_reduced = gb_model_f.predict(X_test2_f)\n",
    "\n",
    "# Évaluation des métriques\n",
    "evaluate_model(y_train_f, y_pred_gb_train_reduced, \"Gradient Boosting\", \"Train\")\n",
    "evaluate_model(y_test1_f, y_pred_gb_test1_reduced, \"Gradient Boosting\", \"Test 1\")\n",
    "evaluate_model(y_test2_f, y_pred_gb_test2_reduced, \"Gradient Boosting\", \"Test 2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b322ca4-12b3-4014-b3f7-54ac459bd1cc",
   "metadata": {},
   "source": [
    "__LSMT__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0bb1f726-0716-4cb3-99ae-9e810f98ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réinitialiser l'index pour éviter les problèmes d'indexation\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "# Réinitialisez également X_test1_scaled, si nécessaire, pour correspondre à y_test1\n",
    "#X_test1_scaled = X_test1_scaled.reset_index(drop=True)\n",
    "y_test1 = y_test1.reset_index(drop=True)\n",
    "\n",
    "#X_test2_scaled = X_test2_scaled.reset_index(drop=True)\n",
    "y_test2 = y_test2.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3a65c2d7-c4cf-4c65-b148-1bbc08d7c7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_seq shape: (6313, 5, 60)\n",
      "y_train_seq shape: (6313,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Définir une fonction pour créer des séquences temporelles\n",
    "def create_sequences(data, target, sequence_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i + sequence_length])\n",
    "        y.append(target[i + sequence_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Paramètres pour les séquences\n",
    "sequence_length = 5  # Utiliser 5 observations passées pour prédire la suivante\n",
    "\n",
    "# Réorganiser vos données\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train, sequence_length)\n",
    "X_test1_seq, y_test1_seq = create_sequences(X_test1_scaled, y_test1, sequence_length)\n",
    "X_test2_seq, y_test2_seq = create_sequences(X_test2_scaled, y_test2, sequence_length)\n",
    "\n",
    "print(f\"X_train_seq shape: {X_train_seq.shape}\")  # [samples, timesteps, features]\n",
    "print(f\"y_train_seq shape: {y_train_seq.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ff6b66c3-a7e2-4797-bace-fde2016d3a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m395/395\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 21.0962 - val_loss: 20.4596 - learning_rate: 1.0000e-05\n",
      "Epoch 2/100\n",
      "\u001b[1m395/395\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 21.5774 - val_loss: 20.4604 - learning_rate: 1.0000e-05\n",
      "Epoch 3/100\n",
      "\u001b[1m395/395\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 21.2147 - val_loss: 20.4621 - learning_rate: 1.0000e-05\n",
      "Epoch 4/100\n",
      "\u001b[1m395/395\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 21.1417 - val_loss: 20.4639 - learning_rate: 1.0000e-05\n",
      "Epoch 5/100\n",
      "\u001b[1m395/395\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 20.8739 - val_loss: 20.4658 - learning_rate: 1.0000e-05\n",
      "Epoch 6/100\n",
      "\u001b[1m395/395\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 21.8677 - val_loss: 20.4683 - learning_rate: 1.0000e-05\n",
      "Epoch 7/100\n",
      "\u001b[1m395/395\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 21.4775 - val_loss: 20.4698 - learning_rate: 5.0000e-06\n",
      "Epoch 8/100\n",
      "\u001b[1m395/395\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 21.4459 - val_loss: 20.4714 - learning_rate: 5.0000e-06\n",
      "Epoch 9/100\n",
      "\u001b[1m395/395\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 21.0757 - val_loss: 20.4728 - learning_rate: 5.0000e-06\n",
      "Epoch 10/100\n",
      "\u001b[1m395/395\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 22.0047 - val_loss: 20.4747 - learning_rate: 5.0000e-06\n",
      "Epoch 11/100\n",
      "\u001b[1m395/395\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 20.7191 - val_loss: 20.4767 - learning_rate: 5.0000e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c5e0090fb0>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    LSTM(128, input_shape=(sequence_length, X_train_seq.shape[2]), activation='relu', return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(64, activation='relu', return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Prédire une seule valeur\n",
    "])\n",
    "\n",
    "# Compiler le modèle\n",
    "lstm_model.compile(optimizer=RMSprop(learning_rate=0.00001), loss='huber')\n",
    "\n",
    "# Entraînement\n",
    "lstm_model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_test1_seq, y_test1_seq),\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "617ad4d1-4083-422f-a4b3-d524e28a86ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "=== LSTM - Train ===\n",
      "Mean Squared Error (MSE): 2084.03\n",
      "Mean Absolute Error (MAE): 22.03\n",
      "Sign Accuracy: 0.53\n",
      "R² Score: 0.00\n",
      "\n",
      "=== LSTM - Test 1 ===\n",
      "Mean Squared Error (MSE): 765.03\n",
      "Mean Absolute Error (MAE): 20.95\n",
      "Sign Accuracy: 0.49\n",
      "R² Score: -0.01\n",
      "\n",
      "=== LSTM - Test 2 ===\n",
      "Mean Squared Error (MSE): 291.55\n",
      "Mean Absolute Error (MAE): 11.38\n",
      "Sign Accuracy: 0.42\n",
      "R² Score: -0.05\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(291.5493939413414),\n",
       " np.float64(11.383961509873059),\n",
       " np.float64(0.418848167539267),\n",
       " -0.05263716387719297)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prédictions\n",
    "y_pred_lstm_train = lstm_model.predict(X_train_seq).flatten()\n",
    "y_pred_lstm_test1 = lstm_model.predict(X_test1_seq).flatten()\n",
    "y_pred_lstm_test2 = lstm_model.predict(X_test2_seq).flatten()\n",
    "\n",
    "# Évaluation\n",
    "evaluate_model(y_train_seq, y_pred_lstm_train, \"LSTM\", \"Train\")\n",
    "evaluate_model(y_test1_seq, y_pred_lstm_test1, \"LSTM\", \"Test 1\")\n",
    "evaluate_model(y_test2_seq, y_pred_lstm_test2, \"LSTM\", \"Test 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d3e44848-82bf-4e3a-905d-ba049bad19c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=6318, stop=8424, step=1)\n"
     ]
    }
   ],
   "source": [
    "print(y_test1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2e81800-e659-4844-9992-69c77a7cb0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spot_id_delta                    1.000000\n",
      "load_forecast                    0.039016\n",
      "coal_power_available             0.044369\n",
      "gas_power_available              0.006614\n",
      "nucelear_power_available         0.039443\n",
      "wind_power_forecasts_average    -0.003468\n",
      "solar_power_forecasts_average   -0.027014\n",
      "wind_power_forecasts_std        -0.015234\n",
      "solar_power_forecasts_std       -0.009145\n",
      "predicted_spot_price             0.043844\n",
      "hour                             0.011612\n",
      "weekday                         -0.004880\n",
      "month                           -0.016374\n",
      "is_weekend                      -0.017934\n",
      "sin_hour                        -0.011829\n",
      "cos_hour                         0.003625\n",
      "sin_weekday                      0.013624\n",
      "cos_weekday                     -0.002205\n",
      "sin_month                        0.033920\n",
      "cos_month                        0.022232\n",
      "Y_gas_mean_6                     0.006855\n",
      "Y_gas_std_6                      0.011539\n",
      "Y_coal_mean_6                    0.044056\n",
      "Y_coal_std_6                     0.007319\n",
      "DELIVERY_START_numeric           0.051041\n",
      "Name: spot_id_delta, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Conversion de DELIVERY_START en format numérique\n",
    "data_cleaned['DELIVERY_START'] = pd.to_datetime(data_cleaned['DELIVERY_START'])\n",
    "data_cleaned['DELIVERY_START_numeric'] = data_cleaned['DELIVERY_START'].astype(int) // 10**9\n",
    "\n",
    "# Conversion des colonnes catégorielles en numériques\n",
    "categorical_cols = data_cleaned.select_dtypes(include=['category']).columns\n",
    "data_cleaned[categorical_cols] = data_cleaned[categorical_cols].apply(lambda x: x.cat.codes)\n",
    "\n",
    "# Sélection des colonnes numériques\n",
    "numeric_data = data_cleaned.select_dtypes(include=['number'])\n",
    "\n",
    "# Calcul de la corrélation avec la variable cible\n",
    "correlation_with_spot_id_delta = numeric_data.corr()['spot_id_delta']\n",
    "print(correlation_with_spot_id_delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "bd1bb91b-085f-4fd2-8e4f-be74311603d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sur-échantillonnage avec SMOTE\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e119024b-7151-46d7-9a22-db3d53e3aa5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'reset_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[184], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Réinitialiser l'index pour éviter les problèmes d'indexation\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y_train_class \u001b[38;5;241m=\u001b[39m \u001b[43my_train_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_index\u001b[49m(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Réinitialisez également X_test1_scaled, si nécessaire, pour correspondre à y_test1\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#X_test1_scaled = X_test1_scaled.reset_index(drop=True)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m y_test1_class \u001b[38;5;241m=\u001b[39m y_test1_class\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'reset_index'"
     ]
    }
   ],
   "source": [
    "# Réinitialiser l'index pour éviter les problèmes d'indexation\n",
    "y_train_class = y_train_class.reset_index(drop=True)\n",
    "\n",
    "# Réinitialisez également X_test1_scaled, si nécessaire, pour correspondre à y_test1\n",
    "#X_test1_scaled = X_test1_scaled.reset_index(drop=True)\n",
    "y_test1_class = y_test1_class.reset_index(drop=True)\n",
    "\n",
    "#X_test2_scaled = X_test2_scaled.reset_index(drop=True)\n",
    "y_test2_class = y_test2_class.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "326841e4-d360-463d-91ae-e1d66d48efc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 6318 is out of bounds for axis 0 with size 6318",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[188], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Étape 2 : Réorganiser les données\u001b[39;00m\n\u001b[0;32m     29\u001b[0m sequence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m---> 30\u001b[0m X_train_seq, y_train_seq \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m X_test1_seq, y_test1_seq \u001b[38;5;241m=\u001b[39m create_sequences(X_test1_scaled, y_test1_class, sequence_length)\n\u001b[0;32m     32\u001b[0m X_test2_seq, y_test2_seq \u001b[38;5;241m=\u001b[39m create_sequences(X_test2_scaled, y_test2_class, sequence_length)\n",
      "Cell \u001b[1;32mIn[188], line 20\u001b[0m, in \u001b[0;36mcreate_sequences\u001b[1;34m(data, target, sequence_length)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m-\u001b[39m sequence_length):\n\u001b[0;32m     19\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend(data[i:i \u001b[38;5;241m+\u001b[39m sequence_length])\n\u001b[1;32m---> 20\u001b[0m     y\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtarget\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(X), np\u001b[38;5;241m.\u001b[39marray(y)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 6318 is out of bounds for axis 0 with size 6318"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Fonction pour transformer y en classification binaire\n",
    "def transform_target_to_class(target):\n",
    "    return np.where(target < 0, -1, 1)\n",
    "\n",
    "# Fonction pour créer des séquences\n",
    "def create_sequences(data, target, sequence_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i + sequence_length])\n",
    "        y.append(target[i + sequence_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Étape 1 : Transformer y en classification binaire\n",
    "y_train_class = transform_target_to_class(y_train)\n",
    "y_test1_class = transform_target_to_class(y_test1)\n",
    "y_test2_class = transform_target_to_class(y_test2)\n",
    "\n",
    "# Étape 2 : Réorganiser les données\n",
    "sequence_length = 5\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_class, sequence_length)\n",
    "X_test1_seq, y_test1_seq = create_sequences(X_test1_scaled, y_test1_class, sequence_length)\n",
    "X_test2_seq, y_test2_seq = create_sequences(X_test2_scaled, y_test2_class, sequence_length)\n",
    "\n",
    "# --- Prétraitement des données ---\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_seq.reshape(X_train_seq.shape[0], -1), y_train_seq)\n",
    "\n",
    "# Reshape après rééquilibrage\n",
    "X_train_balanced = X_train_balanced.reshape(-1, X_train_seq.shape[1], X_train_seq.shape[2])\n",
    "\n",
    "# --- Normalisation des données ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_balanced.reshape(-1, X_train_balanced.shape[2]))\n",
    "X_train_scaled = X_train_scaled.reshape(-1, X_train_balanced.shape[1], X_train_balanced.shape[2])\n",
    "\n",
    "X_test1_scaled = scaler.transform(X_test1_seq.reshape(-1, X_test1_seq.shape[2]))\n",
    "X_test1_scaled = X_test1_scaled.reshape(-1, X_test1_seq.shape[1], X_test1_seq.shape[2])\n",
    "\n",
    "# --- Définir le modèle LSTM ---\n",
    "model = Sequential([\n",
    "    LSTM(128, return_sequences=True, input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2])),\n",
    "    Dropout(0.3),\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Classification binaire\n",
    "])\n",
    "\n",
    "# --- Compilation du modèle ---\n",
    "class_weights = {\n",
    "    0: len(y_train_balanced) / (2 * np.sum(y_train_balanced == 0)),\n",
    "    1: len(y_train_balanced) / (2 * np.sum(y_train_balanced == 1)),\n",
    "}\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# --- Définir les callbacks pour l'entraînement ---\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "]\n",
    "\n",
    "# --- Entraînement du modèle ---\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_balanced,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_test1_scaled, y_test1_seq),\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# --- Évaluation du modèle ---\n",
    "y_pred_proba = model.predict(X_test1_scaled)\n",
    "\n",
    "# Binariser les résultats\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Calculer les scores de classification\n",
    "print(\"Classification Report:\\n\", classification_report(y_test1_seq, y_pred))\n",
    "\n",
    "# AUC-ROC\n",
    "auc_score = roc_auc_score(y_test1_seq, y_pred_proba)\n",
    "print(\"AUC-ROC Score:\", auc_score)\n",
    "\n",
    "# F1-score\n",
    "f1 = f1_score(y_test1_seq, y_pred)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8460f600-a8fe-41cc-a461-6adb2432ed4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.20750339e+07  7.72072200e+02  6.64159191e+03 -4.98783186e+03\n",
      "   1.15047889e+03]\n",
      " [ 1.20786340e+07  2.39948738e+02  7.05675668e+03 -4.90085741e+03\n",
      "   1.27713402e+03]\n",
      " [ 1.20822340e+07 -1.41363546e+03  8.06603614e+03 -5.14064820e+03\n",
      "   1.63616378e+03]\n",
      " ...\n",
      " [ 1.20606334e+07  1.52943055e+04 -5.27928780e+02 -2.21135732e+03\n",
      "  -1.47447722e+03]\n",
      " [ 1.20642334e+07  1.45972796e+04 -1.22032180e+02 -2.34787930e+03\n",
      "  -1.32547805e+03]\n",
      " [ 1.20678334e+07  1.37725093e+04  3.10788687e+02 -2.59561444e+03\n",
      "  -1.15511782e+03]]\n"
     ]
    }
   ],
   "source": [
    "numeric_data = data_cleaned.select_dtypes(include=['float64', 'int64'])\n",
    "numeric_data = numeric_data.dropna()\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "data_reduced = pca.fit_transform(numeric_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "344f2915-862e-4597-b134-225e74799e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.20750339e+07,  7.72072200e+02,  6.64159191e+03,\n",
       "        -4.98783186e+03,  1.15047889e+03],\n",
       "       [ 1.20786340e+07,  2.39948738e+02,  7.05675668e+03,\n",
       "        -4.90085741e+03,  1.27713402e+03],\n",
       "       [ 1.20822340e+07, -1.41363546e+03,  8.06603614e+03,\n",
       "        -5.14064820e+03,  1.63616378e+03],\n",
       "       ...,\n",
       "       [ 1.20606334e+07,  1.52943055e+04, -5.27928780e+02,\n",
       "        -2.21135732e+03, -1.47447722e+03],\n",
       "       [ 1.20642334e+07,  1.45972796e+04, -1.22032180e+02,\n",
       "        -2.34787930e+03, -1.32547805e+03],\n",
       "       [ 1.20678334e+07,  1.37725093e+04,  3.10788687e+02,\n",
       "        -2.59561444e+03, -1.15511782e+03]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed2adf3-e67c-42a9-ae67-fb43874ca655",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
